{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "578cb9a6-315c-4c31-8025-f9e932043845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !where python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52fe4937-713a-49ff-b26e-1c77b9dd4516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-cpp-python\n",
    "# !pip install langchain langchain-community sentence-transformers chromadb\n",
    "# !pip install pypdf requests pydantic tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc6b1176-4980-4053-9018-60229598861a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import LlamaCpp\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "813cbf3a-7ae4-4a2e-98ed-0680e2538438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading llama-2-7b-chat.Q4_K_M.gguf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3985356it [03:11, 20819.94it/s]                                                                                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_path = \"llama-2-7b-chat.Q4_K_M.gguf\"\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"Downloading {model_path}...\")\n",
    "    # You may want to replace the model URL by another of your choice\n",
    "    model_url = \"https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf\"\n",
    "    response = requests.get(model_url, stream=True)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    \n",
    "    with open(model_path, 'wb') as f:\n",
    "        for data in tqdm(response.iter_content(chunk_size=1024), total=total_size//1024):\n",
    "            f.write(data)\n",
    "    print(\"Download complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "116cdc5c-2588-46a9-9df1-80f9b81beac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"docs\", exist_ok=True)\n",
    "\n",
    "# Sample text for demonstration purposes\n",
    "with open(\"docs/sample.txt\", \"w\") as f:\n",
    "    f.write(\"\"\"\n",
    "    Retrieval-Augmented Generation (RAG) is a technique that combines retrieval-based and generation-based approaches\n",
    "    for natural language processing tasks. It involves retrieving relevant information from a knowledge base and then \n",
    "    using that information to generate more accurate and informed responses.\n",
    "    \n",
    "    RAG models first retrieve documents that are relevant to a given query, then use these documents as additional context\n",
    "    for language generation. This approach helps to ground the model's responses in factual information and reduces hallucinations.\n",
    "    \n",
    "    The llama.cpp library is a C/C++ implementation of Meta's LLaMA model, optimized for CPU usage. It allows running LLaMA models\n",
    "    on consumer hardware without requiring high-end GPUs.\n",
    "    \n",
    "    LocalAI is a framework that enables running AI models locally without relying on cloud services. It provides APIs compatible\n",
    "    with OpenAI's interfaces, allowing developers to use their own models with the same code they would use for OpenAI services.\n",
    "    \"\"\")\n",
    "\n",
    "documents = []\n",
    "for file in os.listdir(\"docs\"):\n",
    "    if file.endswith(\".pdf\"):\n",
    "        loader = PyPDFLoader(os.path.join(\"docs\", file))\n",
    "        documents.extend(loader.load())\n",
    "    elif file.endswith(\".txt\"):\n",
    "        loader = TextLoader(os.path.join(\"docs\", file))\n",
    "        documents.extend(loader.load())\n",
    "\n",
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3304260-33d5-43a0-a15e-6c7f5f8b131b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'docs\\\\sample.txt'}, page_content=\"Retrieval-Augmented Generation (RAG) is a technique that combines retrieval-based and generation-based approaches\\n    for natural language processing tasks. It involves retrieving relevant information from a knowledge base and then \\n    using that information to generate more accurate and informed responses.\\n\\n    RAG models first retrieve documents that are relevant to a given query, then use these documents as additional context\\n    for language generation. This approach helps to ground the model's responses in factual information and reduces hallucinations.\\n\\n    The llama.cpp library is a C/C++ implementation of Meta's LLaMA model, optimized for CPU usage. It allows running LLaMA models\\n    on consumer hardware without requiring high-end GPUs.\"), Document(metadata={'source': 'docs\\\\sample.txt'}, page_content=\"The llama.cpp library is a C/C++ implementation of Meta's LLaMA model, optimized for CPU usage. It allows running LLaMA models\\n    on consumer hardware without requiring high-end GPUs.\\n\\n    LocalAI is a framework that enables running AI models locally without relying on cloud services. It provides APIs compatible\\n    with OpenAI's interfaces, allowing developers to use their own models with the same code they would use for OpenAI services.\")]\n"
     ]
    }
   ],
   "source": [
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "712867eb-5187-4437-bde6-808c6cc8c8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba1c61b4-b59c-43ee-b088-b7c9210ddd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCpp(\n",
    "    model_path=model_path,\n",
    "    temperature=0.7,\n",
    "    max_tokens=2000,\n",
    "    n_ctx=4096,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16876445-5e90-42a1-948d-9c55a6dab890",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Answer the question based on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25f14fac-c53d-4fe0-ac50-088ced3ead6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_pipeline = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05beb85f-70ac-4f08-b636-e4aad936bd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question):\n",
    "    start_time = time.time()\n",
    "    result = rag_pipeline({\"query\": question})\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {result['result']}\")\n",
    "    print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
    "    print(\"\\nSource documents:\")\n",
    "    for i, doc in enumerate(result[\"source_documents\"]):\n",
    "        print(f\"Document {i+1}:\")\n",
    "        print(f\"Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "        print(f\"Content: {doc.page_content[:150]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a644940c-7629-43b8-b549-ce363d436864",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rishi\\AppData\\Local\\Temp\\ipykernel_13504\\1799130441.py:3: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = rag_pipeline({\"query\": question})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is RAG and how does it work?\n",
      "Answer: RAG is a technique that combines retrieval-based and generation-based approaches for natural language processing tasks. It works by first retrieving relevant information from a knowledge base, then using this information as additional context for language generation. This helps to ground the model's responses in factual information, reducing hallucinations.\n",
      "Time taken: 61.59 seconds\n",
      "\n",
      "Source documents:\n",
      "Document 1:\n",
      "Source: docs\\sample.txt\n",
      "Content: Retrieval-Augmented Generation (RAG) is a technique that combines retrieval-based and generation-based approaches\n",
      "    for natural language processing ...\n",
      "\n",
      "Document 2:\n",
      "Source: docs\\sample.txt\n",
      "Content: The llama.cpp library is a C/C++ implementation of Meta's LLaMA model, optimized for CPU usage. It allows running LLaMA models\n",
      "    on consumer hardwar...\n",
      "\n",
      "Question: What is llama.cpp?\n",
      "Answer: llama.cpp is a C/C++ implementation of Meta's LLaMA model, optimized for CPU usage. It allows running LLaMA models on consumer hardware without requiring high-end GPUs.\n",
      "Time taken: 20.67 seconds\n",
      "\n",
      "Source documents:\n",
      "Document 1:\n",
      "Source: docs\\sample.txt\n",
      "Content: The llama.cpp library is a C/C++ implementation of Meta's LLaMA model, optimized for CPU usage. It allows running LLaMA models\n",
      "    on consumer hardwar...\n",
      "\n",
      "Document 2:\n",
      "Source: docs\\sample.txt\n",
      "Content: Retrieval-Augmented Generation (RAG) is a technique that combines retrieval-based and generation-based approaches\n",
      "    for natural language processing ...\n",
      "\n",
      "Question: How does LocalAI relate to cloud AI services?\n",
      "Answer: LocalAI is a framework that enables running AI models locally without relying on cloud services. It allows developers to use their own models with the same code they would use for OpenAI services, providing an alternative to cloud-based AI solutions.\n",
      "Time taken: 10.09 seconds\n",
      "\n",
      "Source documents:\n",
      "Document 1:\n",
      "Source: docs\\sample.txt\n",
      "Content: The llama.cpp library is a C/C++ implementation of Meta's LLaMA model, optimized for CPU usage. It allows running LLaMA models\n",
      "    on consumer hardwar...\n",
      "\n",
      "Document 2:\n",
      "Source: docs\\sample.txt\n",
      "Content: Retrieval-Augmented Generation (RAG) is a technique that combines retrieval-based and generation-based approaches\n",
      "    for natural language processing ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask_question(\"What is RAG and how does it work?\")\n",
    "ask_question(\"What is llama.cpp?\")\n",
    "ask_question(\"How does LocalAI relate to cloud AI services?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c339b0c1-6fcb-4d51-9752-289b64b838a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is summary of foreward written by John Crupi in book enterprise integration patterns?\n",
      "Answer: In the forward written by John Crupi, he talks about how the idea for writing this book started during a conversation with Martin Fowler in 2001. He mentions that while working on \"Patterns of Enterprise Application Architecture,\" Martin realized that the book only touched briefly on integration and suggested they write a separate book on message-based integration patterns. Crupi then joined forces with Kyle and Bobby to work on the project, which eventually became \"Enterprise Integration Patterns.\" He highlights the importance of the book's focus on messaging systems and how it will teach readers how to use them effectively in their own applications. Finally, he encourages readers to explore the supporting website for additional information and to provide feedback on the book.\n",
      "Time taken: 64.58 seconds\n",
      "\n",
      "Source documents:\n",
      "Document 1:\n",
      "Source: docs\\enterprise integration patterns.pdf\n",
      "Content: 2001, followed by Gregor in early 2002. The following summer the group submitted two papers \n",
      "for review at the Pattern Languages of Programs (PLoP) co...\n",
      "\n",
      "Document 2:\n",
      "Source: docs\\enterprise integration patterns.pdf\n",
      "Content: them that you may not have been aware of. Finally, the pattern names give you a common \n",
      "vocabulary to efficiently discuss integration design alternati...\n",
      "\n",
      "Document 3:\n",
      "Source: docs\\enterprise integration patterns.pdf\n",
      "Content: immediately following this one in the book. Use the web of interconnected patterns, not the \n",
      "linear list of book pages, to guide you through the mater...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask_question(\"What is summary of foreward written by John Crupi in book enterprise integration patterns?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "181f8fc9-2813-4442-85af-d24b75e158e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What all wrote foreward in book enterprise integration patterns?\n",
      "Answer: The foreword to the book \"Enterprise Integration Patterns\" was written by:\n",
      "Martin Fowler, Kyle Brown, Rachel Reinitz, John Crupi, Bobby Bueker, and Gregor Hohpe.\n",
      "Time taken: 39.72 seconds\n",
      "\n",
      "Source documents:\n",
      "Document 1:\n",
      "Source: docs\\enterprise integration patterns.pdf\n",
      "Content: them that you may not have been aware of. Finally, the pattern names give you a common \n",
      "vocabulary to efficiently discuss integration design alternati...\n",
      "\n",
      "Document 2:\n",
      "Source: docs\\enterprise integration patterns.pdf\n",
      "Content: immediately following this one in the book. Use the web of interconnected patterns, not the \n",
      "linear list of book pages, to guide you through the mater...\n",
      "\n",
      "Document 3:\n",
      "Source: docs\\enterprise integration patterns.pdf\n",
      "Content: • When to send a message, what it should contain, and how to use special message \n",
      "properties  \n",
      "• How to route a message to its ultimate destination ev...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ask_question(\"What all wrote foreward in book enterprise integration patterns?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d530610-646d-447d-b172-581071c627f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
